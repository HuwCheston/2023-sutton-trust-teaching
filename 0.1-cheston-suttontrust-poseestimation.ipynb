{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sutton Trust Music & Science Workshop**\n",
        "\n",
        "Instructor: Huw Cheston, PhD researcher @ Centre for Music & Science, University of Cambridge\n",
        "\n",
        "![ST](https://summerschools.suttontrust.com/wp-content/themes/sutton-trust-summer-programme/assets/img/summer_school_logo.png)\n",
        "\n",
        "Â© Huw Cheston 2023, hwc31@cam.ac.uk"
      ],
      "metadata": {
        "id": "Icp025TcU4Y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pose estimation using neural networks\n",
        "\n",
        "![NN](https://forum.khadas.com/uploads/default/original/2X/f/f96d91501a7613128e0881b6adfdb5cff02bb309.gif)\n",
        "\n",
        "In this workbook, we'll use pose estimation via neural networks to track the motion and movement of dancers and musicians in videos of performances. By immersing themselves in a diverse array of movement examples, these networks become skilled at tracking and mapping the human form in motion, effectively translating dynamic performances into data. This technology holds immense potential, from enhancing training and feedback for artists to enabling immersive digital experiences.\n",
        "\n",
        "In this workbook, we'll use a pose estimation library implemented in [OpenCV](https://opencv.org/). This is much simpler than the example above, which uses [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose), so you shouldn't expect the results to be quite as good. However, it has the advantage of possible to run on the cloud, and is much quicker. We'll rip performances direct from YouTube, so you won't need to download anything beforehand. You also don't need to have any experience of programming to use this workbook, and all the various options will be explained as you go."
      ],
      "metadata": {
        "id": "vj5Su_hjU7HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "**Before you do anything else**, hit the *Play* button below, next to the **Show code** line. You may need to move your mouse for this to appear. Please let me know if you get any errors when running this!"
      ],
      "metadata": {
        "id": "JU8nVPJGVX1k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lmpDuRvRtG",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "!git clone https://github.com/misbah4064/human-pose-estimation-opencv.git\n",
        "!pip install yt-dlp\n",
        "%cd human-pose-estimation-opencv/\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model\n",
        "\n",
        "First things first, go to [YouTube](https://youtube.com) and choose a video of a music or dance performance you want to work with. The video must show **one performer or dancer only**: our algorithm can only work with one performer at a time! Try and choose a video where the camera doesn't move too much, as well.\n",
        "\n",
        "Aside from this, there are no restrictions on genre here, so choose whatever you think might lead to some interesting results! Once you've found a track, copy the link into the field *yt_link* below. It should look something like https://www.youtube.com/watch?v=NlZ0e5FqZEU\n",
        "\n",
        "If the track takes a while to start (maybe it has a long intro), you can use the starting_position slider to skip ahead in the track. So, if the music starts at 10 seconds into the video, you'd set the slider to 10.\n",
        "\n",
        "Once you've set all the parameters, hit the big \"Play\" icon as before and wait a minute for the recording to process. You should see a progress bar appear to let you know how much longer you'll have to wait."
      ],
      "metadata": {
        "id": "5K9izsG9WQEo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPPgVzcH1xVE",
        "cellView": "form"
      },
      "source": [
        "yt_link = 'https://www.youtube.com/watch?v=zV1qLYukTH8' # @param {type:\"string\"}\n",
        "starting_position = 40 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "!yt-dlp $yt_link --force-overwrites -f  \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\" -o youtube.mp4\n",
        "end_pos = starting_position + 10\n",
        "!ffmpeg -y -hide_banner -loglevel error -i youtube.mp4 -ss $starting_position -to $end_pos -c copy cut.mp4\n",
        "\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
        "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
        "\n",
        "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
        "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
        "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
        "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
        "\n",
        "width = 368\n",
        "height = 368\n",
        "inWidth = width\n",
        "inHeight = height\n",
        "\n",
        "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
        "thr = 0.2\n",
        "def poseDetector(frame):\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "    out = net.forward()\n",
        "    out = out[:, :19, :, :]\n",
        "    assert(len(BODY_PARTS) == out.shape[1])\n",
        "    points = []\n",
        "    data = {}\n",
        "    for i, part in zip(range(len(BODY_PARTS)), BODY_PARTS):\n",
        "        heatMap = out[0, i, :, :]\n",
        "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "        data[part] = (int(x), int(y)) if conf > thr else None\n",
        "    for pair in POSE_PAIRS:\n",
        "        partFrom = pair[0]\n",
        "        partTo = pair[1]\n",
        "        assert(partFrom in BODY_PARTS)\n",
        "        assert(partTo in BODY_PARTS)\n",
        "        idFrom = BODY_PARTS[partFrom]\n",
        "        idTo = BODY_PARTS[partTo]\n",
        "        if points[idFrom] and points[idTo]:\n",
        "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
        "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "    t, _ = net.getPerfProfile()\n",
        "    return frame, data\n",
        "\n",
        "def frame_iter(capture, description):\n",
        "  def _iterator():\n",
        "      while capture.grab():\n",
        "          yield capture.retrieve()[1]\n",
        "  return tqdm(\n",
        "      _iterator(),\n",
        "      desc=description,\n",
        "      total=int(capture.get(cv.CAP_PROP_FRAME_COUNT)),\n",
        "  )\n",
        "\n",
        "cap = cv.VideoCapture('cut.mp4')\n",
        "ret, frame = cap.read()\n",
        "frame_height, frame_width, _ = frame.shape\n",
        "out = cv.VideoWriter('output.mp4', cv.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
        "all_data = []\n",
        "frame_num = 1\n",
        "for frame in frame_iter(cap, 'Processing video ...'):\n",
        "  output, motion = poseDetector(frame)\n",
        "  motion['frame'] = frame_num\n",
        "  all_data.append(motion)\n",
        "  out.write(output)\n",
        "  frame_num += 1\n",
        "out.release()\n",
        "print(\"... done!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a graph\n",
        "\n",
        "Once you're happy with how the pose estimation is working, you can press the play button on the next cell to create a graph showing the change in X and Y positions of the detected body parts for every frame of the video."
      ],
      "metadata": {
        "id": "zEYZAhs3W2f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df = pd.DataFrame(all_data)\n",
        "df = df.fillna(value=np.nan)\n",
        "\n",
        "res = []\n",
        "for col in df.columns[:-1]:\n",
        "  r = []\n",
        "  for idx, val in df[col].items():\n",
        "    try:\n",
        "      x, y = val\n",
        "    except:\n",
        "      x, y = np.nan, np.nan\n",
        "    finally:\n",
        "      r.append({f'{col}_x': x, f'{col}_y': y})\n",
        "  fmt = pd.DataFrame(r)\n",
        "  for c in [f'{col}_x', f'{col}_y']:\n",
        "    fmt[c] = fmt[c].diff()\n",
        "  res.append(fmt)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(nrows=len(res), ncols=1, sharex=True, sharey=False, figsize=(8, 1*len(res)))\n",
        "for n, (part, a) in enumerate(zip(res, ax.flatten())):\n",
        "  for col in part.columns:\n",
        "    label = col.split('_')[-1].title()\n",
        "    a.plot(part.index, part[col], label=label)\n",
        "  a.set(title=part.columns[-1].replace('_y', '').title())\n",
        "  if n == 0:\n",
        "    a.legend()\n",
        "\n",
        "fig.supxlabel('Frames')\n",
        "fig.supylabel('Change [px]')\n",
        "fig.subplots_adjust(hspace=0.5, left=0.1, bottom=0.05)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_tML_y6fOxPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the output\n",
        "\n",
        "Congratulations, you just used a neural network for the first time! How do the results sound? You can try different combinations of parameters (or different videos) by changing the parameters above and pressing the \"Play\" button once again.\n",
        "\n",
        "If you can't think of which tracks to use, you can try the following:\n",
        "\n",
        "*   Ballet: https://www.youtube.com/watch?v=zV1qLYukTH8\n",
        "*   Jazz: https://www.youtube.com/watch?v=-Zi5Xq-1jSU"
      ],
      "metadata": {
        "id": "c8o-7uWCXCyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion questions\n",
        "\n",
        "1.   Do particular styles of music or dance lend themselves better to pose estimation? Which styles work better, and what connects them?\n",
        "2.   What are some of the potential applications of this technology, both in research and in practice?"
      ],
      "metadata": {
        "id": "l2XSNB8VXiTD"
      }
    }
  ]
}