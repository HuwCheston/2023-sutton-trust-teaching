{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxY-FPWvg8v0"
      },
      "source": [
        "# **Sutton Trust Music & Science Workshop**\n",
        "\n",
        "Instructor: Huw Cheston, PhD researcher @ Centre for Music & Science, University of Cambridge\n",
        "\n",
        "![ST](https://summerschools.suttontrust.com/wp-content/themes/sutton-trust-summer-programme/assets/img/summer_school_logo.png)\n",
        "\n",
        "© Huw Cheston 2023, hwc31@cam.ac.uk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19sleM2Q2hO4"
      },
      "source": [
        "# Automatic music transcription using neural networks\n",
        "\n",
        "![NN](https://magenta.tensorflow.org/assets/transcription-with-transformers/mt3_diagram.png)\n",
        "\n",
        "If you've ever tried to listen to a recording of a piece of music and transcribe it into notation, you'll know that this can be quite a time consuming and difficult process. Neural networks can be applied to enable automatic transcription of many forms of recorded music, and can even work on polyphonic instruments like the piano. By digesting a wealth of musical examples, these networks become adept at recognizing notes, chords, and rhythms, essentially learning how to translate sound waves into written symbols – much like translating a language. This technology opens up exciting possibilities for musicians, composers, and musicologists, offering them the tools to analyze, recreate, and build upon musical compositions in ways that were once intricate and time-consuming.\n",
        "\n",
        "\n",
        "In this workbook, we'll going to use an automatic music transcription library called [BasicPitch](https://github.com/spotify/basic-pitch), which was developed by [Spotify](https://www.spotify.com). We'll be working with tracks ripped directly from YouTube, so you won't need to download anything beforehand. You also don't need to have any experience of programming to use this workbook, and all the various options will be explained as you go.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5qdgLAhLJj"
      },
      "source": [
        "## Setup\n",
        "\n",
        "**Before you do anything else**, hit the *Play* button below, next to the **Show code** line. You may need to move your mouse for this to appear. Please let me know if you get any errors when running this! This may take a minute or so to run, and you'll see some code in the window as this happens: you'll need to wait until the wheel stops spinning and is replaced by a green tick before moving on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cVu0u25fbAVp"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!apt install ffmpeg\n",
        "!sudo apt install -y fluidsynth\n",
        "!pip install basic-pitch yt-dlp pretty_midi pyfluidsynth\n",
        "\n",
        "import tensorflow as tf\n",
        "import basic_pitch.inference as bp\n",
        "from basic_pitch import ICASSP_2022_MODEL_PATH\n",
        "import pretty_midi\n",
        "from pretty_midi.pretty_midi import PrettyMIDI\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import collections\n",
        "import pandas as pd\n",
        "\n",
        "SAMPLE_RATE = 44100\n",
        "HOP_LENGTH = 512\n",
        "BASIC_PITCH_MODEL = tf.saved_model.load(str(ICASSP_2022_MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz0BBvdJoaXC"
      },
      "source": [
        "## Run the model\n",
        "\n",
        "First things first, go to [YouTube](https://youtube.com) and choose a recording you want to work with. We're looking for recordings that contain **solo piano only** – no other instruments allowed!\n",
        "\n",
        "Aside from this, there are no restrictions on genre here, so choose whatever you think might lead to some interesting results! You could try a classical piano piece, an unaccompanied jazz solo, or an arrangement of a pop song. Once you've found a track, copy the link into the field *yt_link* below. It should look something like https://www.youtube.com/watch?v=NlZ0e5FqZEU\n",
        "\n",
        "If the track takes a while to start (maybe it has a long intro), you can use the starting_position slider to skip ahead in the track. So, if the music starts at 10 seconds into the video, you'd set the slider to 10.\n",
        "\n",
        "Next, you'll need to experiment with setting the other parameters. These stand for:\n",
        "\n",
        "* note_threshold: Minimum energy required for a note to be considered present. If you're finding that you get lots of repeated notes, increase this value!\n",
        "* minimum_note_length: The minimum allowed note length in milliseconds.\n",
        "* minimum_freq: Minimum allowed audio frequency, in Hz. You'll probably want to set this fairly low: remember that the low A on a piano is ~27Hz!\n",
        "* maximum_freq: Maximum allowed audio frequency, in Hz.\n",
        "\n",
        "Once you've set all the parameters, hit the big \"Play\" icon as before and wait a minute for the recording to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlypNeJNgwaU",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "yt_link = 'https://www.youtube.com/watch?v=X5Sg0WGy9YA' # @param {type:\"string\"}\n",
        "starting_position = 1 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "note_threshold = 0.8 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "minimum_note_length = 100 # @param {type:\"slider\", min:0, max:1000, step:10}\n",
        "minimum_freq = 100 # @param {type:\"slider\", min:100, max:44100, step:100}\n",
        "maximum_freq = 8300 # @param {type:\"slider\", min:100, max:44100, step:100}\n",
        "if minimum_freq > maximum_freq:\n",
        "  raise ValueError('Minimum frequency cannot be above maximum frequency!')\n",
        "\n",
        "end_position = starting_position + 60\n",
        "!yt-dlp -f \"bestaudio\" --extract-audio --force-overwrites --audio-format mp3 -o youtube.mp3 --postprocessor-args \"-ar 44100\" $yt_link\n",
        "!ffmpeg -y -hide_banner -loglevel error -i youtube.mp3 -ss $starting_position -to $end_position -c copy cut.mp3\n",
        "\n",
        "print('Starting BasicPitch ...')\n",
        "model_output, midi_data, note_events = bp.predict(\n",
        "    'cut.mp3',\n",
        "    BASIC_PITCH_MODEL,\n",
        "    minimum_frequency=minimum_freq,\n",
        "    maximum_frequency=maximum_freq,\n",
        "    onset_threshold=note_threshold,\n",
        "    frame_threshold=0.3,\n",
        "    minimum_note_length=minimum_note_length,\n",
        "    melodia_trick=True,\n",
        "    multiple_pitch_bends=False\n",
        ")\n",
        "print('... done!')\n",
        "\n",
        "print(f'↓↓↓ Listen to synthesized MIDI below (may take a second to appear) ↓↓↓')\n",
        "midi_data.instruments[0].program = 0\n",
        "midi_data.remove_invalid_notes()\n",
        "Audio(midi_data.fluidsynth(fs=SAMPLE_RATE), rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHplST-vABdB"
      },
      "source": [
        "## Create a graph\n",
        "\n",
        "Once you're happy with how the beat tracking is working, you can press the play button on the next cell to create a graph showing the distribution of the pitch and duration of the first 100 notes in the recording."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yerUG-aK8PwN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def midi_to_notes(pm) -> pd.DataFrame:\n",
        "  instrument = pm.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "  # Sort the notes by start time\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    notes['pitch'].append(note.pitch)\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    prev_start = start\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n",
        "\n",
        "def plot_piano_roll(notes: pd.DataFrame, count = None):\n",
        "  if count:\n",
        "    title = f'First {count} notes'\n",
        "  else:\n",
        "    title = f'Whole track'\n",
        "    count = len(notes['pitch'])\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
        "  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
        "  plt.plot(\n",
        "      plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker='.')\n",
        "  plt.xlabel('Time [s]')\n",
        "  plt.ylabel('Pitch')\n",
        "  plt.title(title)\n",
        "\n",
        "raw_notes = midi_to_notes(midi_data)\n",
        "get_note_names = np.vectorize(pretty_midi.note_number_to_name)\n",
        "sample_note_names = get_note_names(raw_notes['pitch'])\n",
        "raw_notes['note_name'] = get_note_names(raw_notes['pitch'])\n",
        "raw_notes['note_name'] = raw_notes['note_name'].str.replace('\\d+', '')\n",
        "plot_piano_roll(raw_notes, count=100)\n",
        "\n",
        "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(10, 4))\n",
        "  sns.histplot(notes.sort_values(by='note_name'), x=\"note_name\", bins=12, ax=ax[0])\n",
        "  ax[0].set(xlabel='Note name')\n",
        "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 12), ax=ax[1])\n",
        "  ax[1].set(xlabel='Note duration [s]', ylabel='')\n",
        "\n",
        "plot_distributions(raw_notes.head(100))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cvLcCXn0tkJ"
      },
      "source": [
        "## Evaluate the output\n",
        "\n",
        "Congratulations, you just used a neural network for the first time! How do the results sound? You can try different combinations of parameters (or different videos) by changing the parameters above and pressing the \"Play\" button once again.\n",
        "\n",
        "If you can't think of which tracks to use, you can try the following:\n",
        "\n",
        "*   Classical: https://www.youtube.com/watch?v=o5dL-65mKe0\n",
        "*   Impressionist: https://www.youtube.com/watch?v=cVMGwPDP-Yk\n",
        "*   Pop: https://www.youtube.com/watch?v=b3E6E6hYSSI\n",
        "*   Jazz: https://www.youtube.com/watch?v=X5Sg0WGy9YA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvOByVQk16-Z"
      },
      "source": [
        "## Discussion questions\n",
        "\n",
        "1.   Which combination of parameters lead to the best results? Which combination leads to the worst results?\n",
        "2.   Do parameters that work well for one recording transfer to another? Why (or why not)?\n",
        "3.   Are the results consistent across different genres? What about different songs?\n",
        "4.   What might the distribution graphs above tell us about a particular performance, and performance in general?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}